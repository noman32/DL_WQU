{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62b340a",
   "metadata": {},
   "source": [
    "# Linear Regression and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39397bf",
   "metadata": {},
   "source": [
    "## 1. Linear Models and Linear Regression\n",
    "\n",
    "In deep learning, before we build powerful neural networks, we must understand their most basic form: the **linear model**.  \n",
    "Think of this section as our launchpad—the point where we first define the idea of a \"model\" in machine learning.\n",
    "\n",
    "We'll explore:\n",
    "- What linear regression is,\n",
    "- How it relates to neural networks,\n",
    "- And how it applies to real-world problems like predicting concrete strength.\n",
    "\n",
    "#### **What is Linear Regression?**\n",
    "\n",
    "**Linear regression** is one of the simplest—and most important—techniques in supervised learning.\n",
    "\n",
    "It models the relationship between **inputs** and **outputs** by assuming a **straight-line relationship** between them.\n",
    "\n",
    "Mathematically, this is expressed as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = XW + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $X$ = Input features (tensor)  \n",
    "- $W$ = Weights (learnable parameters)  \n",
    "- $b$ = Bias term  \n",
    "- $\\hat{y}$ = Predicted output\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "> \"We’re trying to find the best straight line that explains the relationship between what we know ($X$) and what we want to predict ($\\hat{y}$).\"\n",
    "\n",
    "This is the starting point for all of deep learning—even the most complex neural networks are built from stacks of linear transformations like this.\n",
    "\n",
    "#### **Connection to Neural Networks**\n",
    "\n",
    "What happens if we take this linear model and place it inside a neural network?\n",
    "\n",
    "Actually, a **neural network with no hidden layers and no activation functions** behaves **exactly like a linear regression model**.  \n",
    "It computes:\n",
    "\n",
    "$$\n",
    "\\hat{y} = XW + b\n",
    "$$\n",
    "\n",
    "No non-linearity, no depth—just a direct, weighted transformation of inputs.\n",
    "\n",
    "> **Key Insight**:  \n",
    "> Linear models are the *core units* of deep learning.  \n",
    "> What makes neural networks powerful is that we **stack** these units and introduce **non-linear functions** between them.\n",
    "\n",
    "In this notebook, we’ll start from this fundamental building block and eventually expand toward more complex networks.\n",
    "\n",
    "#### **Flow of a Linear Model**\n",
    "\n",
    "**Input Features** ($X$)  \n",
    "⬇️  \n",
    "**Weighted Sum** ($XW$)  \n",
    "⬇️  \n",
    "**Add Bias** ($+b$)  \n",
    "⬇️  \n",
    "**Prediction** ($\\hat{y}$)\n",
    "\n",
    "\n",
    "#### **Real-World Applications of Linear Regression**\n",
    "\n",
    "Linear regression is not just a teaching example—it powers real applications:\n",
    "\n",
    "- 🏠 **Predicting house prices** based on area, location, and age\n",
    "- 👨‍💼 **Estimating salary** based on experience and education\n",
    "- 🏗️ **Predicting concrete strength** based on material composition (our case study!)\n",
    "\n",
    "> **Reflect & Connect:**  \n",
    "> Think about other real-world examples where you expect a *linear relationship*—can you name one?\n",
    "\n",
    "In the next section, we’ll take this mathematical idea and express it as code using PyTorch tensors.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f45bc99",
   "metadata": {},
   "source": [
    "## 2. Mathematical Formulation of a Linear Model\n",
    "\n",
    "Now that we understand the intuition behind linear regression, let’s look more closely at the **mathematical structure**.\n",
    "\n",
    "We know the prediction rule is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = XW + b\n",
    "$$\n",
    "\n",
    "But what do these symbols actually mean?\n",
    "\n",
    "#### **Breaking Down the Equation**\n",
    "\n",
    "| Symbol      | Meaning                      | Typical Shape        |\n",
    "|-------------|------------------------------|-----------------------|\n",
    "| $X$         | Input features (matrix)      | $(n, d)$ = n samples × d features |\n",
    "| $W$         | Weights (vector/matrix)      | $(d, 1)$              |\n",
    "| $b$         | Bias (scalar or vector)      | Scalar or $(n, 1)$    |\n",
    "| $\\hat{y}$   | Predicted outputs            | $(n, 1)$              |\n",
    "\n",
    "So this model performs a **linear transformation** of the inputs, followed by a **bias shift**.  \n",
    "The multiplication $XW$ gives us a **weighted sum of features**, and $b$ lets us adjust the output.\n",
    "\n",
    "- $XW$: **Weighted Sums** → shape $(n \\times 1)$  \n",
    "\n",
    "#### **Example: Manual Calculation**\n",
    "\n",
    "Let’s compute a concrete example with:\n",
    "- 3 data points\n",
    "- 2 input features per point\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "W =\n",
    "\\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "1.5\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "b = 2\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\hat{y} = XW + b =\n",
    "\\begin{bmatrix}\n",
    "1 \\cdot 0.5 + 2 \\cdot 1.5 + 2 \\\\\n",
    "3 \\cdot 0.5 + 4 \\cdot 1.5 + 2 \\\\\n",
    "5 \\cdot 0.5 + 6 \\cdot 1.5 + 2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "6 \\\\\n",
    "11 \\\\\n",
    "16\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "> **Reflection Prompt:**  \n",
    "> Why must $X$ and $W$ have these exact shapes?  \n",
    "> What would go wrong if $W$ had shape (1, d) instead of (d, 1)?\n",
    "\n",
    "This is a key insight when building models with PyTorch—**tensor shapes must match** for the operations to work!\n",
    "\n",
    "In the next section, we’ll move from this math into actual code: we’ll define a `predict()` function that performs this operation using PyTorch tensors.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2459e22",
   "metadata": {},
   "source": [
    "## 3. Predict Function on a Tiny Dataset\n",
    "\n",
    "So far, we've defined our model mathematically as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = XW + b\n",
    "$$\n",
    "\n",
    "Now it's time to **bring this equation to life** using PyTorch.\n",
    "\n",
    "To keep things intuitive, we’ll start with a **tiny toy dataset** that’s easy to follow.  \n",
    "This will help us understand how predictions are made using matrix multiplication.\n",
    "\n",
    "We’ll:\n",
    "1. Create a simple dataset `X` with a few input values  \n",
    "2. Manually define parameters `W` and `b`  \n",
    "3. Implement a `predict(X)` function  \n",
    "4. Compute predictions $\\hat{y}$ using our linear model\n",
    "\n",
    "#### **Before the Code: What's Happening?**\n",
    "\n",
    "Let’s summarize the steps:\n",
    "\n",
    "- Create:\n",
    "  - A 4-sample input tensor `X` (1D feature)\n",
    "  - A target output `Y` that follows a linear pattern\n",
    "- Initialize:\n",
    "  - A simple weight `W = 0.5`\n",
    "  - A bias `b = 0`\n",
    "- Define a function:\n",
    "  - `predict(X)` that returns $XW + b$\n",
    "- Print the predicted output\n",
    "\n",
    "This helps build intuition for how weights and bias **shape the model’s output**—and it sets the stage for learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce2412de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "tensor([[0.5000],\n",
      "        [1.0000],\n",
      "        [1.5000],\n",
      "        [2.0000]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Step 1: Tiny toy dataset\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])  # Inputs\n",
    "Y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])  # Ground truth\n",
    "\n",
    "# Step 2: Manually initialized weight and bias\n",
    "W = torch.tensor([[0.5]], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "# Step 3: Define predict function (ŷ = XW + b)\n",
    "def predict(X):\n",
    "    return torch.matmul(X, W) + b\n",
    "\n",
    "# Step 4: Make predictions\n",
    "y_pred = predict(X)\n",
    "\n",
    "# Step 5: Show results\n",
    "print(\"Predictions:\")\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed826f9",
   "metadata": {},
   "source": [
    "> What did we just see?\n",
    "\n",
    "The model applied a simple **linear transformation** to the inputs:\n",
    "- Multiply each input by 0.5\n",
    "- Add 0\n",
    "\n",
    "This produces outputs: `[0.5, 1.0, 1.5, 2.0]` which are **significantly lower than** the true targets: `[2.0, 4.0, 6.0, 8.0]`\n",
    "\n",
    "✅ This highlights that our model parameters **are not yet optimal**.  \n",
    "Later, we’ll learn how to automatically update `W` and `b` using gradient descent to **minimize the difference** between predicted and actual values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7eb600",
   "metadata": {},
   "source": [
    "## 4. Loss Functions for Regression\n",
    "\n",
    "Once a model makes predictions, we need a way to **measure how well it's doing**.  \n",
    "That’s where a **loss function** comes in.\n",
    "\n",
    "In supervised learning, a loss function compares the model's predictions $\\hat{y}$ to the true target values $y$ and returns a single number—a **score** that tells us how far off we are.\n",
    "\n",
    "> 🎯 The goal of training is to **minimize this loss**.\n",
    "\n",
    "#### **Mean Squared Error (MSE)**\n",
    "\n",
    "For regression tasks, the most common loss function is the **Mean Squared Error (MSE)**.\n",
    "\n",
    "It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y_i$ = the actual value  \n",
    "- $\\hat{y}_i$ = the predicted value  \n",
    "- $n$ = number of examples\n",
    "\n",
    "#### **How It Works:**\n",
    "\n",
    "1. Take the **difference** between prediction and actual value  \n",
    "2. **Square** the difference (to penalize large errors more strongly)  \n",
    "3. **Average** across all data points\n",
    "\n",
    "This gives us a single loss value representing overall prediction error.\n",
    "\n",
    "#### **Code Preview: What Will We Do?**\n",
    "\n",
    "We’ll:\n",
    "- Define a custom `mse_loss()` function in PyTorch\n",
    "- Pass in a small set of predictions and true values\n",
    "- Compute the MSE and print the result\n",
    "\n",
    "This is a good **sanity check** to verify our understanding of the loss calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d88d19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.25\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Mean Squared Error (MSE) function\n",
    "def mse_loss(predictions, targets):\n",
    "    return torch.mean((predictions - targets) ** 2)\n",
    "\n",
    "# Example: ground truth vs. predictions\n",
    "Y_true = torch.tensor([[2.0], [4.0], [6.0], [8.0]])      # True targets\n",
    "Y_pred = torch.tensor([[2.5], [3.5], [5.5], [8.5]])      # Model predictions\n",
    "\n",
    "# Compute loss\n",
    "loss = mse_loss(Y_pred, Y_true)\n",
    "print(\"MSE Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065af81",
   "metadata": {},
   "source": [
    "> What Did We See?\n",
    "\n",
    "Our function computed the **average squared difference** between predicted and actual values.  \n",
    "This tells us how far off the model is—**lower MSE = better performance**.\n",
    "\n",
    "In our example, we get a small positive number showing modest prediction error.\n",
    "\n",
    "#### **Aside: Other Common Regression Losses**\n",
    "\n",
    "- **Mean Absolute Error (MAE):** Uses absolute differences; more robust to outliers  \n",
    "- **Huber Loss:** Combines MSE and MAE behavior; often used when we want stability with outliers\n",
    "\n",
    "\n",
    "In a upcoming section, we’ll use this MSE function inside a **training loop** to guide the model in updating its parameters (ϕ).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c6a830",
   "metadata": {},
   "source": [
    "## 5. Preparing the Data and Model\n",
    "\n",
    "Now that we’ve defined how the model works and how to measure its performance, it’s time to **prepare the data and the model parameters** for training.\n",
    "\n",
    "This is a critical setup step. A model with poor data preparation or unstable initialization will struggle to learn, even if the math is correct.\n",
    "\n",
    "We’ll:\n",
    "1. Load the Concrete dataset\n",
    "2. Normalize the input features for stability\n",
    "3. Split the data into training and test sets\n",
    "4. Initialize the model parameters: weights `W` and bias `b`\n",
    "\n",
    "#### **Data & Model Setup: High-Level Plan**\n",
    "\n",
    "```text\n",
    "Load CSV\n",
    "    ↓\n",
    "Extract inputs & targets\n",
    "    ↓\n",
    "Standardize features (mean=0, std=1)\n",
    "    ↓\n",
    "Split into 80% train, 20% test\n",
    "    ↓\n",
    "Initialize W and b with gradient tracking\n",
    "```\n",
    "\n",
    "Let’s now implement this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8fe7bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 824\n",
      "Test set size: 206\n",
      "Weight shape: torch.Size([8, 1])\n",
      "Bias shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Load dataset\n",
    "data = pd.read_csv(r\"D:\\OneDrive\\WorldQuant_DeepLearning\\Nomans_Work_DL_WQU\\concrete+compressive+strength\\Concrete_Data.csv\")  # Adjust path as needed\n",
    "\n",
    "# 2. Separate features and target\n",
    "inputs = data.iloc[:, :-1].values       # First 8 columns = features\n",
    "targets = data.iloc[:, -1].values.reshape(-1, 1)  # Last column = target\n",
    "\n",
    "# 3. Normalize input features\n",
    "scaler = StandardScaler()\n",
    "inputs_scaled = scaler.fit_transform(inputs)\n",
    "\n",
    "# 4. Convert to PyTorch tensors\n",
    "inputs_tensor = torch.tensor(inputs_scaled, dtype=torch.float32)\n",
    "targets_tensor = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "# 5. Shuffle and split into training (80%) and test (20%) sets\n",
    "torch.manual_seed(42)\n",
    "n_samples = inputs_tensor.shape[0]\n",
    "indices = torch.randperm(n_samples)\n",
    "split_idx = int(n_samples * 0.8)\n",
    "\n",
    "train_indices = indices[:split_idx]\n",
    "test_indices = indices[split_idx:]\n",
    "\n",
    "X_train = inputs_tensor[train_indices]\n",
    "y_train = targets_tensor[train_indices]\n",
    "X_test = inputs_tensor[test_indices]\n",
    "y_test = targets_tensor[test_indices]\n",
    "\n",
    "# 6. Initialize model parameters\n",
    "num_features = X_train.shape[1]  # Should be 8\n",
    "W = torch.randn((num_features, 1), requires_grad=True)\n",
    "W.data *= 0.01  # Scale down initial values\n",
    "b = torch.zeros((1,), requires_grad=True)\n",
    "\n",
    "# 7. Confirm everything is set up\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])\n",
    "print(\"Weight shape:\", W.shape)\n",
    "print(\"Bias shape:\", b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943274ca",
   "metadata": {},
   "source": [
    "#### **Why Normalize the Data?**\n",
    "\n",
    "The input features vary widely in scale—e.g., cement can be over 500, while superplasticizer is often below 5.\n",
    "\n",
    "These differences lead to:\n",
    "- Large gradients\n",
    "- Exploding updates\n",
    "- Slow or unstable training\n",
    "\n",
    "✅ Standardization ensures:\n",
    "- Mean = 0\n",
    "- Standard deviation = 1  \n",
    "And helps the model **learn more reliably**.\n",
    "\n",
    "#### **Why Split the Dataset?**\n",
    "\n",
    "To evaluate how well the model **generalizes**, we set aside a **test set** that the model never sees during training.\n",
    "\n",
    "- **Training set**: used to update weights  \n",
    "- **Test set**: used to measure generalization\n",
    "\n",
    "We use a typical **80-20 split**.\n",
    "\n",
    "#### **Why Initialize with Small Weights?**\n",
    "\n",
    "If weights are too large at the beginning, the model might:\n",
    "- Produce massive outputs\n",
    "- Suffer from exploding gradients\n",
    "\n",
    "We initialize:\n",
    "- `W` as small random values (`~N(0, 0.01)`)\n",
    "- `b` as zero\n",
    "\n",
    "Also, by setting `requires_grad=True`, PyTorch will track gradients for these variables during training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236230f7",
   "metadata": {},
   "source": [
    "## 6. Understanding Parameters in PyTorch\n",
    "\n",
    "In the previous section, we initialized two model parameters: **weights** `W` and **bias** `b`.\n",
    "\n",
    "Let’s now take a closer look at what they represent, how they are shaped, and how PyTorch tracks their updates during training.\n",
    "\n",
    "#### **What Do the Parameters Represent?**\n",
    "\n",
    "- **Weights `W`**  \n",
    "  Each weight controls the influence of one input feature on the model’s output.\n",
    "  > “How much does this feature matter in predicting concrete strength?”\n",
    "\n",
    "- **Bias `b`**  \n",
    "  A constant that shifts all predictions up or down—like adjusting the baseline.\n",
    "\n",
    "Together, these parameters define the **linear function**:\n",
    "$$\n",
    "\\hat{y} = XW + b\n",
    "$$\n",
    "\n",
    "#### **Understanding the Shapes**\n",
    "\n",
    "Since we have 8 input features, our parameter shapes must match the matrix multiplication rules:\n",
    "\n",
    "| Parameter | Shape     | Description                        |\n",
    "|-----------|-----------|------------------------------------|\n",
    "| `X`       | (n, 8)    | `n` samples, each with 8 features  |\n",
    "| `W`       | (8, 1)    | One weight per feature             |\n",
    "| `b`       | (1,)      | A single scalar bias               |\n",
    "| $\\hat{y}$ | (n, 1)    | One predicted value per example    |\n",
    "\n",
    "#### **Shape Flow Schematic**\n",
    "\n",
    "```text\n",
    "Input X:           (n × 8)\n",
    "Weights W:         (8 × 1)\n",
    "-------------------------\n",
    "XW result:       (n × 1)\n",
    "+ Bias b:        (broadcasted to n × 1)\n",
    "-------------------------\n",
    "Predictions ŷ:   (n × 1)\n",
    "```\n",
    "✅ PyTorch handles this **broadcasting automatically** when we add `b`.\n",
    "\n",
    "#### **Why `requires_grad=True`?**\n",
    "\n",
    "To train the model using **gradient descent**, PyTorch needs to compute the **gradients of the loss** with respect to the parameters:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\phi} = \\{W, b\\}\n",
    "$$\n",
    "\n",
    "We tell PyTorch which tensors to **track for gradients** by setting:\n",
    "```python\n",
    "W = torch.randn((8, 1), requires_grad=True)\n",
    "b = torch.zeros((1,), requires_grad=True)\n",
    "```\n",
    "\n",
    "This enables Automatic Differentiation in PyTorch: PyTorch builds a **computation graph** behind the scenes and stores all operations involving `W` and `b`.  \n",
    "When we later call `.backward()` on the loss, PyTorch uses the **chain rule** to compute:\n",
    "\n",
    "$$\n",
    "\\nabla_\\phi L\n",
    "$$\n",
    "\n",
    "This gives us the **gradient of the loss** with respect to each parameter.\n",
    "\n",
    "#### **Summary**\n",
    "\n",
    "- `W` and `b` are the **only parameters** in this model  \n",
    "- Their shapes must align with the **input-output structure**  \n",
    "- With `requires_grad=True`, PyTorch **tracks gradients** to help reduce the loss  \n",
    "\n",
    "In the next section, we’ll introduce **gradient descent** — the core algorithm that uses these gradients to **update `W` and `b`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a3ef8",
   "metadata": {},
   "source": [
    "## 7. Gradient Descent – Conceptual Overview\n",
    "\n",
    "In the previous section, we saw that PyTorch can track how the loss depends on each parameter ϕ.  \n",
    "But knowing the gradient is only part of the story—we still need to use that information to **actually improve the model**.\n",
    "\n",
    "That’s where **gradient descent** comes in.\n",
    "\n",
    "#### **The Goal: Minimize the Loss**\n",
    "\n",
    "Our model is only useful if it makes **accurate predictions**.  \n",
    "To measure how far off it is, we use a **loss function** $L[\\phi]$, which outputs a scalar value—  \n",
    "**the total error across the training data**.\n",
    "\n",
    "> 🎯 Our goal is to **find the values of ϕ that make this loss as small as possible.**\n",
    "\n",
    "This is called an **optimization problem**.\n",
    "\n",
    "#### **The Role of the Gradient**\n",
    "\n",
    "The gradient is like a compass—it tells us:\n",
    "\n",
    "- 📉 **Which direction** will reduce the loss (the sign of the derivative)\n",
    "- 📏 **How steeply** the loss changes (the magnitude)\n",
    "\n",
    "Each parameter has its own gradient:\n",
    "- $\\frac{\\partial L}{\\partial W}$ tells us how much to change the weights\n",
    "- $\\frac{\\partial L}{\\partial b}$ tells us how much to change the bias\n",
    "\n",
    "\n",
    "#### **The Gradient Descent Rule**\n",
    "\n",
    "We update the parameters using the following rule:\n",
    "\n",
    "$$\n",
    "\\phi \\leftarrow \\phi - \\alpha \\cdot \\nabla_\\phi L\n",
    "$$\n",
    "\n",
    "Let’s unpack it:\n",
    "\n",
    "| Symbol         | Meaning                                |\n",
    "|----------------|----------------------------------------|\n",
    "| $\\phi$         | A model parameter (e.g., W or b)       |\n",
    "| $\\nabla_\\phi L$| Gradient of the loss wrt. that parameter |\n",
    "| $\\alpha$       | Learning rate (how big a step we take) |\n",
    "\n",
    "#### **Step-by-Step Update (Per Parameter)**\n",
    "\n",
    "```text\n",
    "1. Compute loss  L[ϕ]      ← How far off are our predictions?\n",
    "2. Compute gradient ∇ϕ L   ← How should we change W and b?\n",
    "3. Take a step:            ← Update each parameter:\n",
    "   ϕ ← ϕ - α ⋅ ∇ϕ L\n",
    "4. Repeat for many epochs\n",
    "```\n",
    "```text\n",
    "   X, y\n",
    "    ↓\n",
    " forward pass: compute ŷ = XW + b\n",
    "    ↓\n",
    "     loss = MSE(ŷ, y)\n",
    "    ↓\n",
    " backward pass: compute gradients ∇W, ∇b\n",
    "    ↓\n",
    " update: \n",
    "   W ← W - α ⋅ ∇W\n",
    "   b ← b - α ⋅ ∇b\n",
    "```\n",
    "\n",
    "#### **Choosing the Right Learning Rate**\n",
    "\n",
    "The learning rate $\\alpha$ controls **how fast** we move toward the minimum:\n",
    "\n",
    "| Learning Rate ($\\alpha$) | Behavior                      |\n",
    "|--------------------------|-------------------------------|\n",
    "| Too small                | Learns very slowly            |\n",
    "| Too large                | Overshoots or diverges        |\n",
    "| Just right               | Steady improvement in loss    |\n",
    "\n",
    "✅ The **learning rate is a hyperparameter** — we choose it **before training**.\n",
    "\n",
    "#### **Visual Intuition**\n",
    "\n",
    "Imagine you're standing on a **hilly surface**, trying to reach the **lowest point**:\n",
    "\n",
    "- The **gradient** tells you which way is downhill  \n",
    "- The **learning rate** tells you how big each step should be  \n",
    "\n",
    "📌 If steps are **too big**, you might **overshoot** or fall off a cliff  \n",
    "📌 If steps are **too small**, you'll take forever to reach the valley\n",
    "\n",
    "#### **Key Idea:**\n",
    "\n",
    "- Gradients are **directional signals**, not solutions  \n",
    "- We still need to decide **how to apply them** — that's what **gradient descent** gives us\n",
    "\n",
    "✅ **In PyTorch**:\n",
    "\n",
    "1. After computing the loss, we call `.backward()` to **populate gradients**  \n",
    "2. Then we **manually update the parameters** using these gradients inside a **training loop**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d8870",
   "metadata": {},
   "source": [
    "## 8. Training Loop – Manual Gradient Descent\n",
    "\n",
    "Now that we understand the gradient descent update rule, it’s time to build a complete training loop and put all the moving parts together.\n",
    "\n",
    "This is where learning *actually happens*.  \n",
    "Our model starts with random weights and bias, and this loop will gradually adjust them to **minimize the loss** using the gradients.\n",
    "\n",
    "#### **What Does a Training Loop Do?**\n",
    "\n",
    "In every training **epoch** (iteration), we perform the following steps:\n",
    "\n",
    "1. **Forward pass** – Compute predictions: $\\hat{y} = XW + b$  \n",
    "2. **Loss computation** – Measure how far off the predictions are  \n",
    "3. **Backward pass** – Compute gradients via `.backward()`  \n",
    "4. **Parameter update** – Adjust `W` and `b` using gradient descent  \n",
    "5. **Gradient reset** – Clear old gradients before the next epoch\n",
    "\n",
    "#### **Code block to be implemented later:**\n",
    "```python\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 1. Predict\n",
    "    y_pred = predict(X_train)\n",
    "\n",
    "    # 2. Compute loss\n",
    "    loss = mse_loss(y_pred, y_train)\n",
    "\n",
    "    # 3. Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # 4. Update weights\n",
    "    with torch.no_grad():\n",
    "        W -= learning_rate * W.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    # 5. Reset gradients\n",
    "    W.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "```\n",
    "#### **Why Use `torch.no_grad()`**\n",
    "\n",
    "When updating `W` and `b`, we **don’t want PyTorch to track** these operations in the computation graph.  \n",
    "Wrapping the update step in `torch.no_grad()` disables gradient tracking inside that block.\n",
    "\n",
    "#### **Why Reset Gradients?**\n",
    "\n",
    "By default, PyTorch **accumulates gradients** after each `.backward()` call.  \n",
    "So we must **reset them manually** using:\n",
    "\n",
    "```python\n",
    "W.grad.zero_()\n",
    "b.grad.zero_()\n",
    "```\n",
    "❗ If we don’t clear the gradients, PyTorch will add new gradients on top of the old ones, leading to incorrect updates during training.\n",
    "\n",
    "✅ Let’s now implement the actual training loop below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12f204ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss = 1523.0864\n",
      "Epoch 20: Loss = 1466.9370\n",
      "Epoch 30: Loss = 1413.0427\n",
      "Epoch 40: Loss = 1361.3132\n",
      "Epoch 50: Loss = 1311.6598\n",
      "Epoch 60: Loss = 1263.9987\n",
      "Epoch 70: Loss = 1218.2491\n",
      "Epoch 80: Loss = 1174.3339\n",
      "Epoch 90: Loss = 1132.1782\n",
      "Epoch 100: Loss = 1091.7117\n"
     ]
    }
   ],
   "source": [
    "# Set learning rate and number of training epochs\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # 1. Forward pass: compute predictions\n",
    "    y_pred = predict(X_train)\n",
    "    \n",
    "    # 2. Compute loss\n",
    "    loss = mse_loss(y_pred, y_train)\n",
    "    \n",
    "    # 3. Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # 4. Update parameters\n",
    "    with torch.no_grad():\n",
    "        W -= learning_rate * W.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    # 5. Zero gradients\n",
    "    W.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a79b151",
   "metadata": {},
   "source": [
    "> 🧠 What Did We Just Build?\n",
    "\n",
    "This training loop applies **gradient descent manually** for 100 epochs.  \n",
    "Over time, the loss should go down, showing that our model is improving.\n",
    "\n",
    "✅ By the end, our weight and bias should be better suited to the data  \n",
    "—meaning: the model has **learned** from the input-output examples.\n",
    "\n",
    "Next, we’ll visualize this improvement by plotting the **loss over epochs**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c1324",
   "metadata": {},
   "source": [
    "## 9. Plotting Training Loss Over Epochs\n",
    "\n",
    "#### **Why Plot Training Loss?**\n",
    "\n",
    "During training, it's important to **monitor the loss over time** to understand how well the model is learning.\n",
    "\n",
    "#### **What Does the Loss Curve Tell Us?**\n",
    "\n",
    "- If the loss **decreases smoothly**, your model is learning.\n",
    "- If the loss **oscillates**, **increases**, or **stays flat**, it may indicate issues:\n",
    "  - Learning rate too high or too low\n",
    "  - Poor initialization\n",
    "  - Bugs in the training loop\n",
    "\n",
    "#### **Good vs. Bad Behavior**\n",
    "\n",
    "| Behavior                     | Interpretation                     |\n",
    "|-----------------------------|-------------------------------------|\n",
    "| Steadily decreasing loss    | ✅ Model is learning properly        |\n",
    "| Increasing or unstable loss | ⚠️ Check learning rate or gradients |\n",
    "| Flat loss                   | ⚠️ Model may be stuck (not learning) |\n",
    "\n",
    "#### **Goal**\n",
    "\n",
    "We will:\n",
    "1. Track loss values during training\n",
    "2. Plot them after training using **matplotlib**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0150a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss = 1052.8652\n",
      "Epoch 20: Loss = 1015.5741\n",
      "Epoch 30: Loss = 979.7750\n",
      "Epoch 40: Loss = 945.4076\n",
      "Epoch 50: Loss = 912.4145\n",
      "Epoch 60: Loss = 880.7400\n",
      "Epoch 70: Loss = 850.3309\n",
      "Epoch 80: Loss = 821.1362\n",
      "Epoch 90: Loss = 793.1071\n",
      "Epoch 100: Loss = 766.1965\n",
      "Epoch 110: Loss = 740.3594\n",
      "Epoch 120: Loss = 715.5526\n",
      "Epoch 130: Loss = 691.7347\n",
      "Epoch 140: Loss = 668.8658\n",
      "Epoch 150: Loss = 646.9078\n",
      "Epoch 160: Loss = 625.8241\n",
      "Epoch 170: Loss = 605.5796\n",
      "Epoch 180: Loss = 586.1406\n",
      "Epoch 190: Loss = 567.4748\n",
      "Epoch 200: Loss = 549.5510\n",
      "Epoch 210: Loss = 532.3395\n",
      "Epoch 220: Loss = 515.8117\n",
      "Epoch 230: Loss = 499.9404\n",
      "Epoch 240: Loss = 484.6992\n",
      "Epoch 250: Loss = 470.0626\n",
      "Epoch 260: Loss = 456.0068\n",
      "Epoch 270: Loss = 442.5083\n",
      "Epoch 280: Loss = 429.5450\n",
      "Epoch 290: Loss = 417.0952\n",
      "Epoch 300: Loss = 405.1386\n",
      "Epoch 310: Loss = 393.6553\n",
      "Epoch 320: Loss = 382.6264\n",
      "Epoch 330: Loss = 372.0339\n",
      "Epoch 340: Loss = 361.8603\n",
      "Epoch 350: Loss = 352.0888\n",
      "Epoch 360: Loss = 342.7033\n",
      "Epoch 370: Loss = 333.6886\n",
      "Epoch 380: Loss = 325.0298\n",
      "Epoch 390: Loss = 316.7128\n",
      "Epoch 400: Loss = 308.7237\n",
      "Epoch 410: Loss = 301.0497\n",
      "Epoch 420: Loss = 293.6781\n",
      "Epoch 430: Loss = 286.5969\n",
      "Epoch 440: Loss = 279.7947\n",
      "Epoch 450: Loss = 273.2603\n",
      "Epoch 460: Loss = 266.9829\n",
      "Epoch 470: Loss = 260.9523\n",
      "Epoch 480: Loss = 255.1589\n",
      "Epoch 490: Loss = 249.5931\n",
      "Epoch 500: Loss = 244.2459\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize list to store loss values\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.001\n",
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_pred = predict(X_train)\n",
    "    loss = mse_loss(y_pred, y_train)\n",
    "    losses.append(loss.item())  # Track the loss\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        W -= learning_rate * W.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    W.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# Plot loss curve\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss over Epochs\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8960dd",
   "metadata": {},
   "source": [
    "✅ This plot helps us understand whether our training procedure is working and whether the learning rate and model parameters are reasonable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f282b8",
   "metadata": {},
   "source": [
    "## 10. Evaluating Model Performance\n",
    "\n",
    "Training is complete—but how do we know whether the model actually **learned something meaningful**?\n",
    "\n",
    "We now test our model on **unseen data** using the **test set** that we held out earlier.  \n",
    "This step evaluates the model’s ability to **generalize**, not just memorize.\n",
    "\n",
    "#### **Why Do We Need a Test Set?**\n",
    "\n",
    "Training loss only tells us how well the model fits the data it has already seen.\n",
    "\n",
    "But a model that performs well on training data might:\n",
    "- Perform poorly on unseen data (⚠️ overfitting), or\n",
    "- Be too simple and fail on both (⚠️ underfitting)\n",
    "\n",
    "The **test loss** gives us a better picture of **real-world performance**.\n",
    "\n",
    "#### **Evaluation Steps**\n",
    "\n",
    "1. Use the final model (W, b) to predict on `X_test`\n",
    "2. Compute MSE between predicted and true test values\n",
    "3. Compare test loss to training loss\n",
    "\n",
    "#### **Interpreting the Results**\n",
    "\n",
    "| Observation                      | Diagnosis            |\n",
    "|----------------------------------|----------------------|\n",
    "| Both train and test loss high   | ❌ Underfitting       |\n",
    "| Train loss low, test loss high  | ❌ Overfitting        |\n",
    "| Both reasonably low             | ✅ Generalization     |\n",
    "\n",
    "> ✅ We aim for **low loss on both sets**. That’s a sign of a model that has *learned general patterns* in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbcaef3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 236.4342\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test set\n",
    "\n",
    "# Step 1: Predict on test data\n",
    "y_test_pred = predict(X_test)\n",
    "\n",
    "# Step 2: Compute MSE on test set\n",
    "test_loss = mse_loss(y_test_pred, y_test)\n",
    "\n",
    "# Step 3: Print the result\n",
    "print(f\"Test Loss: {test_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b95e50",
   "metadata": {},
   "source": [
    "> ✅ If the test loss is **close to the training loss**, this suggests that the model generalizes well.\n",
    "\n",
    "If the test loss is **significantly higher**, the model may have **memorized the training data** instead of learning real patterns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6564e7cc",
   "metadata": {},
   "source": [
    "## 11. Reflection and Summary\n",
    "\n",
    "🎉 You’ve just completed a full, hands-on walkthrough of **manual linear regression in PyTorch**!\n",
    "\n",
    "More than just writing code, you’ve built an understanding of:\n",
    "- What a model actually *is*\n",
    "- How it learns\n",
    "- How to monitor and evaluate that learning\n",
    "\n",
    "#### **What We Built**\n",
    "\n",
    "We developed a **complete training pipeline** from scratch:\n",
    "\n",
    "- A **linear model**: $\\hat{y} = XW + b$\n",
    "- A custom **loss function**: Mean Squared Error (MSE)\n",
    "- A manual **training loop** that:\n",
    "  - Computes predictions\n",
    "  - Measures loss\n",
    "  - Calculates gradients via `.backward()`\n",
    "  - Updates parameters via gradient descent\n",
    "  - Tracks and plots loss\n",
    "\n",
    "#### **What We Learned**\n",
    "\n",
    "| Core Concept           | What You Now Know                                                 |\n",
    "|------------------------|-------------------------------------------------------------------|\n",
    "| Linear Regression      | The simplest building block of neural networks                   |\n",
    "| PyTorch Tensors        | How to represent and manipulate data, weights, and gradients     |\n",
    "| Gradient Descent       | How a model learns from loss and adjusts its parameters          |\n",
    "| Training/Test Split    | How to evaluate generalization, not just memorization            |\n",
    "| Loss Visualization     | How to monitor and debug the learning process                    |\n",
    "| Model Evaluation       | How to detect overfitting, underfitting, or generalization       |\n",
    "\n",
    "\n",
    "#### **Why This Matters**\n",
    "\n",
    "This notebook has shown you that even the most advanced deep learning models are built on **simple components like this one**.  \n",
    "A neural network without hidden layers *is* a linear model.\n",
    "\n",
    "> ✅ Understanding these fundamentals gives us the clarity to build and debug much deeper architectures later.\n",
    "\n",
    "\n",
    "#### **Flow of Algorithm**\n",
    "\n",
    "```text\n",
    "Input Data (X)         Target (y)\n",
    "      ↓                     ↓\n",
    "Forward Pass:        ŷ = XW + b\n",
    "      ↓\n",
    "Loss Computation:    MSE(ŷ, y)\n",
    "      ↓\n",
    "Backward Pass:       .backward()\n",
    "      ↓\n",
    "Gradient Descent:    W ← W - α ⋅ ∇W,   b ← b - α ⋅ ∇b\n",
    "      ↓\n",
    "Repeat over Epochs → Track & Plot Loss → Evaluate on Test Set\n",
    "```\n",
    "#### **What’s Coming Next**\n",
    "\n",
    "This notebook introduced a model that can only draw **straight lines**.  \n",
    "But most real-world problems involve **non-linear relationships**!\n",
    "\n",
    "#### **Up Next, We Will:**\n",
    "\n",
    "- 🔍 Explore the **limits** of linear models  \n",
    "- ⚡ Introduce **activation functions** like ReLU  \n",
    "- 🧱 Build our first **neural network with a hidden layer**\n",
    "\n",
    "✅ We’re now ready to go from **lines to layers**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f1944",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
