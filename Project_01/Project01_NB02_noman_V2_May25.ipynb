{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62b340a",
   "metadata": {},
   "source": [
    "# Project 2 - Linear Regression and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c45d88a",
   "metadata": {},
   "source": [
    "<b> üìñ 1. What is Linear Regression? </b>\n",
    "\n",
    "‚úÖ Objective:\n",
    "\n",
    "- Connect the idea that a **neural network without hidden layers and without activations** is simply a **linear model**.\n",
    "\n",
    "- Provide an **intuitive explanation** of linear regression with **real-world examples**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd579d",
   "metadata": {},
   "source": [
    "<b> What is Linear Regression? </b>\n",
    "\n",
    "üîµ **Linear Regression** is one of the simplest forms of supervised learning.\n",
    "\n",
    "In its basic form, it models the relationship between **input(s)** and **output** by assuming a **straight-line relationship**:\n",
    "\n",
    "üîµ **Connection to Neural Networks:**\n",
    "\n",
    "- A **neural network with no hidden layers and no activation functions** behaves exactly like a **linear regression model**.\n",
    "- It simply computes a **weighted sum** of inputs plus a bias.\n",
    "\n",
    "> **Key Insight:**  \n",
    "> Linear models are the building blocks of deep learning. If we stack multiple layers and apply activation functions, we move from simple lines to very complex, nonlinear functions!\n",
    "\n",
    "\n",
    "\n",
    "<b> üèó Real-world Examples: </b>\n",
    "- Predicting **house prices** based on size.\n",
    "- Estimating **salary** based on years of experience.\n",
    "- Predicting **concrete strength** based on material composition (our dataset!).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb70832d",
   "metadata": {},
   "source": [
    "=== Revised\n",
    "## 1. Introduction: What is Linear Regression?\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Understand that a **neural network without hidden layers or activation functions** is equivalent to a **linear regression model**.\n",
    "- Develop an **intuitive understanding** of linear regression through real-world examples.\n",
    "\n",
    "### 1.1 What is Linear Regression?\n",
    "\n",
    "Linear Regression is one of the most fundamental approaches in supervised learning. It models the relationship between **input variables** and an **output variable** by assuming a **linear (straight-line)** relationship.\n",
    "\n",
    "Mathematically, this is represented as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = XW + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( X \\) is the input tensor (data),\n",
    "- \\( W \\) are the weights,\n",
    "- \\( b \\) is the bias term,\n",
    "- \\( \\hat{y} \\) is the predicted output.\n",
    "\n",
    "### 1.2 Connection to Neural Networks\n",
    "\n",
    "A **neural network with no hidden layers and no activation functions** behaves exactly like a **linear regression model**. It simply computes a weighted sum of the input features and adds a bias.\n",
    "\n",
    "> **Key Insight:**  \n",
    "> Linear models are the foundation of deep learning. When we stack layers and introduce activation functions, we evolve from simple lines to highly expressive, non-linear models.\n",
    "\n",
    "### 1.3 Real-World Examples of Linear Relationships\n",
    "\n",
    "Some examples where linear regression is applicable include:\n",
    "\n",
    "- Predicting **house prices** based on size.\n",
    "- Estimating **salary** based on years of experience.\n",
    "- Predicting **concrete strength** based on material composition ‚Äî which is the focus of our current dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6829a839",
   "metadata": {},
   "source": [
    "<b> 2. Mathematical Formulation of Linear Model </b>\n",
    "\n",
    "The prediction formula for a linear model is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = XW + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "| Term       | Meaning                   | Shape                 |\n",
    "|:-----------|:--------------------------|:----------------------|\n",
    "| $X$        | Input features matrix     | $(n, d)$              |\n",
    "| $W$        | Weights vector            | $(d, 1)$              |\n",
    "| $b$        | Bias (scalar or $(n, 1)$) | Scalar or broadcasted |\n",
    "| $\\hat{y}$  | Predicted outputs         | $(n, 1)$              |\n",
    "\n",
    "\n",
    "‚úÖ In words:  \n",
    "- **Multiply** the input features by the weight vector.  \n",
    "- **Add** the bias.  \n",
    "- You get the **predicted output**!\n",
    "\n",
    "<b> üß† Example:</b>\n",
    "\n",
    "Suppose we have:\n",
    "- 3 data points (rows)\n",
    "- 2 features each (columns)\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "W = \\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "1.5\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "b = 2\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\hat{y} = XW + b =\n",
    "\\begin{bmatrix}\n",
    "1 \\times 0.5 + 2 \\times 1.5 + 2 \\\\\n",
    "3 \\times 0.5 + 4 \\times 1.5 + 2 \\\\\n",
    "5 \\times 0.5 + 6 \\times 1.5 + 2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "6 \\\\\n",
    "11 \\\\\n",
    "16\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755fbf95",
   "metadata": {},
   "source": [
    "üìñ Section 3: Predict Function (Tiny Dataset)\n",
    "\n",
    "### ‚úÖ Objective:\n",
    "\n",
    "- Move from **theory to a small working example**  \n",
    "- Manually define a `predict(X)` function  \n",
    "- Show the **very first forward computation** step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df32865",
   "metadata": {},
   "source": [
    "#### üß† Markdown Cell 3.1: Predict Function Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0303f4",
   "metadata": {},
   "source": [
    "#### 3.1 Predict Function\n",
    "\n",
    "Now, let's move from formulas to actual code!\n",
    "\n",
    "We will define a **predict** function that calculates:\n",
    "\n",
    "$$\n",
    "\\hat{y} = XW + b\n",
    "$$\n",
    "\n",
    "where $ X $ is our input tensor, $ W $ are the weights, and $ b $ is the bias.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ Remember:  \n",
    "- $ W $ and $ b $ are **parameters** we want to learn later during training.\n",
    "- For now, we'll just manually set them to random values.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f358d039",
   "metadata": {},
   "source": [
    "#### üñ•Ô∏è Code Cell 3.1: Predict Function on Tiny Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc06950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "tensor([[0.5000],\n",
      "        [1.0000],\n",
      "        [1.5000],\n",
      "        [2.0000]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Tiny toy dataset\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "Y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
    "\n",
    "# Randomly initialized weights and bias\n",
    "W = torch.tensor([[0.5]], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "# Predict function\n",
    "def predict(X):\n",
    "    return torch.matmul(X, W) + b\n",
    "\n",
    "# Test prediction\n",
    "y_pred = predict(X)\n",
    "print(\"Predictions:\")\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59800b",
   "metadata": {},
   "source": [
    "#### ‚úçÔ∏è Practice Cell 3.1 (Optional small exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44abe8a",
   "metadata": {},
   "source": [
    "#### üõ†Ô∏è Practice Exercise 3.1\n",
    "\n",
    "‚úÖ Your Task:\n",
    "\n",
    "- Modify the weight **W** and bias **b** manually.\n",
    "- Predict again.\n",
    "- Observe how the output changes.\n",
    "\n",
    "üîî *Hint:* Try changing W to 2.0 and b to 1.0 and see if predictions improve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc5695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "160b6059",
   "metadata": {},
   "source": [
    "## üìñ Section 4: MSE Loss (Mean Squared Error)\n",
    "\n",
    "### ‚úÖ Objective:\n",
    "\n",
    "- Introduce **loss functions** in regression problems  \n",
    "- Focus mainly on **Mean Squared Error (MSE)**  \n",
    "- Briefly mention alternatives like **Mean Absolute Error (MAE)** and **Huber Loss**  \n",
    "- **Manually implement** a simple MSE function in code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c73a1",
   "metadata": {},
   "source": [
    "#### üß† Markdown Cell 4.1: Loss Functions for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560d15db",
   "metadata": {},
   "source": [
    "### 4.1 Loss Functions for Regression\n",
    "\n",
    "In supervised learning, we need a way to measure **how good or bad** our model's predictions are compared to the actual targets.  \n",
    "This measurement is done using a **loss function**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ Mean Squared Error (MSE)\n",
    "\n",
    "The **Mean Squared Error (MSE)** is the most common loss function for **regression** tasks.\n",
    "\n",
    "Mathematically, it is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y_i$ = true target value  \n",
    "- $\\hat{y}_i$ = model prediction  \n",
    "- $n$ = number of examples\n",
    "\n",
    "‚úÖ **In simple terms:**\n",
    "- Take the **difference** between prediction and true value  \n",
    "- **Square** it (to make all differences positive)  \n",
    "- **Average** the result across all data points\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ Other Loss Functions (just for awareness)\n",
    "\n",
    "- **Mean Absolute Error (MAE):**  \n",
    "  Takes the absolute difference ‚Äî less sensitive to large errors\n",
    "\n",
    "- **Huber Loss:**  \n",
    "  A combination of MSE and MAE ‚Äî more robust to outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad877ea2",
   "metadata": {},
   "source": [
    "#### üñ•Ô∏è Code Cell 4.1: Implement MSE Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746662a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Mean Squared Error (MSE) function\n",
    "def mse_loss(predictions, targets):\n",
    "    return torch.mean((predictions - targets)**2)\n",
    "\n",
    "# Test with tiny dataset\n",
    "# Ground truth\n",
    "Y_true = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
    "\n",
    "# Random predictions\n",
    "Y_pred = torch.tensor([[2.5], [3.5], [5.5], [8.5]])\n",
    "\n",
    "# Compute loss\n",
    "loss = mse_loss(Y_pred, Y_true)\n",
    "print(\"MSE Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e884b220",
   "metadata": {},
   "source": [
    "#### ‚úçÔ∏è Practice Cell 4.1 (Small Exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed1734",
   "metadata": {},
   "source": [
    "#### üõ†Ô∏è Practice Exercise 4.1\n",
    "\n",
    "‚úÖ Your Task:\n",
    "\n",
    "- Modify the `Y_pred` values manually to make them **closer** to `Y_true`.\n",
    "- Recompute the MSE loss.\n",
    "- **Observe**: Does the loss decrease?\n",
    "\n",
    "üîî *Hint:* \n",
    "- Try setting `Y_pred = [[2.1], [4.0], [6.1], [7.9]]` \n",
    "- Calculate and compare!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd290b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c98942e",
   "metadata": {},
   "source": [
    "## üìñ Section 5: Training/Test Split\n",
    "\n",
    "### ‚úÖ Objective:\n",
    "\n",
    "- Introduce **why we need train/test splits**  \n",
    "- Explain the concept of **generalization vs. overfitting**  \n",
    "- Actually split the **Concrete dataset** into **80% training** and **20% testing**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c27519",
   "metadata": {},
   "source": [
    "#### üß† Markdown Cell 5.1: Why Split into Training and Test Sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c44ef94",
   "metadata": {},
   "source": [
    "### 5.1 Why Split into Training and Test Sets?\n",
    "\n",
    "‚úÖ When we train a machine learning model, we want it to perform **well not only on the data it saw during training, but also on new, unseen data**.\n",
    "\n",
    "This is called **generalization**.\n",
    "\n",
    "---\n",
    "\n",
    "üîµ **Training Set:**\n",
    "- Used to learn the model's weights (parameters).\n",
    "- Model sees this data during optimization.\n",
    "\n",
    "üîµ **Test Set:**\n",
    "- Used to evaluate how well the model generalizes.\n",
    "- Model **should NOT** see this data during training!\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ Without a test set, we might create models that **memorize** (overfit) the training data without truly learning patterns that generalize.\n",
    "\n",
    "---\n",
    "**Example:**  \n",
    "Imagine learning a set of 10 quiz questions by heart.  \n",
    "If the final exam gives you **the same questions**, you'll ace it (training success).  \n",
    "But if you get **new questions**, your true understanding is tested (generalization)!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0044c7ca",
   "metadata": {},
   "source": [
    "#### üñ•Ô∏è Code Cell 5.1: Split Concrete Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7599ea8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 824\n",
      "Test set size: 206\n"
     ]
    }
   ],
   "source": [
    "# Assume 'inputs_tensor' and 'targets_tensor' are already created (from Notebook 1)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Total number of samples\n",
    "n_samples = inputs_tensor.shape[0]\n",
    "\n",
    "# Shuffle indices\n",
    "indices = torch.randperm(n_samples)\n",
    "\n",
    "# 80-20 split\n",
    "split_idx = int(n_samples * 0.8)\n",
    "\n",
    "train_indices = indices[:split_idx]\n",
    "test_indices = indices[split_idx:]\n",
    "\n",
    "# Create training and testing sets\n",
    "X_train = inputs_tensor[train_indices]\n",
    "y_train = targets_tensor[train_indices]\n",
    "\n",
    "X_test = inputs_tensor[test_indices]\n",
    "y_test = targets_tensor[test_indices]\n",
    "\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2c6b25",
   "metadata": {},
   "source": [
    "#### ‚úçÔ∏è Practice Cell 5.1 (Small Exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef0aab",
   "metadata": {},
   "source": [
    "#### üõ†Ô∏è Practice Exercise 5.1\n",
    "\n",
    "‚úÖ Your Task:\n",
    "\n",
    "- Print:\n",
    "  - First 5 examples of `X_train` and `y_train`.\n",
    "  - First 5 examples of `X_test` and `y_test`.\n",
    "- Confirm that no data leakage happened (training and testing sets are separate).\n",
    "\n",
    "üîî *Hint:* \n",
    "- Use `print(X_train[:5])`, `print(y_train[:5])`, etc.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6450bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c10430bb",
   "metadata": {},
   "source": [
    "## üìö Section 6: Manual Parameter Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7423d23",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "\n",
    "- Introduce the concept of **parameters**: weights ($W$) and bias ($b$)  \n",
    "- Manually **initialize parameters** as PyTorch tensors  \n",
    "- Set `requires_grad=True` to enable **gradient tracking**  \n",
    "- üõ†Ô∏è **Practice:** Follow the instructions below and write your own code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b458a13",
   "metadata": {},
   "source": [
    "#### üß† Markdown Cell 6.1: Why Initialize Parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a634c620",
   "metadata": {},
   "source": [
    "### 6.1 Manual Initialization of Model Parameters\n",
    "\n",
    "‚úÖ To perform linear regression, our model needs **two sets of parameters**:\n",
    "- **Weights (W)** ‚Äî these determine how important each input feature is.\n",
    "- **Bias (b)** ‚Äî this shifts the output up or down.\n",
    "\n",
    "---\n",
    "\n",
    "üîµ In PyTorch:\n",
    "- We initialize weights and bias as **torch tensors**.\n",
    "- We must set **`requires_grad=True`** so that PyTorch can compute **gradients** automatically later during training.\n",
    "\n",
    "---\n",
    "\n",
    "### üìè Parameter Shapes:\n",
    "\n",
    "Suppose we are using **only 1 feature** from the Concrete dataset (for simplicity):\n",
    "\n",
    "| Parameter | Shape | Example |\n",
    "|:---|:---|:---|\n",
    "| Weight (W) | (1, 1) | One weight per input feature |\n",
    "| Bias (b) | (1,) | Single bias |\n",
    "\n",
    "‚úÖ Later, when we move to multiple features, the shape of `W` will be `(num_features, 1)`.\n",
    "\n",
    "---\n",
    "\n",
    "> **Key point:**  \n",
    "> - Random initial weights are fine.\n",
    "> - Training will adjust them based on data!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639ff220",
   "metadata": {},
   "source": [
    "#### ‚úçÔ∏è Practice Task 6.1 (instead of full code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06855e7",
   "metadata": {},
   "source": [
    "#### üõ†Ô∏è Practice Exercise 6.1\n",
    "\n",
    "‚úÖ Your Task:\n",
    "\n",
    "- Manually initialize the model parameters:\n",
    "  - Weight tensor `W`\n",
    "  - Bias tensor `b`\n",
    "- Both should have:\n",
    "  - Random initial values (small numbers)\n",
    "  - `requires_grad=True` so PyTorch tracks gradients\n",
    "\n",
    "üîî *Hint:* \n",
    "- Use `torch.randn()` for random numbers.\n",
    "- Example: `W = torch.randn((1, 1), requires_grad=True)`\n",
    "\n",
    "üî• Quick Tip:\n",
    "\n",
    "‚úÖ You can think of `W` and `b` as **variables** whose values we want to \"tune\" during training.\n",
    "\n",
    "‚úÖ PyTorch will automatically compute **how much to change W and b** to make predictions better during each training step!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb425b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3b99bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: tensor([[-0.6641]], requires_grad=True)\n",
      "Bias: tensor([-0.3682], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# üñ•Ô∏è Expected Solution (only for Instructor ‚Äî not shown to students)\n",
    "\n",
    "import torch\n",
    "\n",
    "# Random weight and bias with gradient tracking\n",
    "W = torch.randn((1, 1), requires_grad=True)\n",
    "b = torch.randn((1,), requires_grad=True)\n",
    "\n",
    "print(\"Weight:\", W)\n",
    "print(\"Bias:\", b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394780ae",
   "metadata": {},
   "source": [
    "## üìö Section 7: Gradient Descent Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d291f9",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "\n",
    "- Explain what **gradient descent** is in **intuitive terms**\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Key Concepts to Cover:\n",
    "\n",
    "- **Loss Minimization**  \n",
    "  Our goal is to reduce the loss ‚Äî make predictions as close as possible to the true values.\n",
    "\n",
    "- **Gradients**  \n",
    "  Gradients tell us **which direction** to move the parameters (weights and bias) and **how much**.  \n",
    "  - The **sign** tells the direction.  \n",
    "  - The **magnitude** tells how steep the slope is.\n",
    "\n",
    "- **Learning Rate**  \n",
    "  The learning rate is the **step size** we use when updating parameters using the gradients.  \n",
    "  - Too small: training is slow  \n",
    "  - Too large: may overshoot or diverge\n",
    "\n",
    "---\n",
    "\n",
    "üìå *Gradient descent is like finding the lowest point in a valley by taking small steps downhill, guided by the slope.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea7f432",
   "metadata": {},
   "source": [
    "#### üß† Markdown Cell 7.1: What is Gradient Descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3ba08",
   "metadata": {},
   "source": [
    "### 7.1 Gradient Descent Concept\n",
    "\n",
    "‚úÖ Once we have a **model** (like our linear model) and a **loss function** (like MSE),  \n",
    "we need a way to **optimize** the model parameters ($W$ and $b$) to **minimize the loss**.\n",
    "\n",
    "This is where **gradient descent** comes in!\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ What is Gradient Descent?\n",
    "\n",
    "- It is an **optimization algorithm**\n",
    "- It updates the model parameters in the **direction that reduces the loss the fastest**\n",
    "- It uses the **gradient (slope)** of the loss function to decide how to change $W$ and $b$\n",
    "\n",
    "---\n",
    "\n",
    "### üìà How Gradient Descent Works\n",
    "\n",
    "For each parameter (e.g., a weight $w$):\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\alpha \\times \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = Learning rate (a small positive number)  \n",
    "- $\\frac{\\partial L}{\\partial w}$ = Derivative (gradient) of the loss with respect to the weight\n",
    "\n",
    "‚úÖ This means:\n",
    "- If the loss increases when you increase $w$, you should **decrease** $w$\n",
    "- If the loss decreases when you increase $w$, you should **increase** $w$\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ Learning Rate\n",
    "\n",
    "- Controls **how big** each update step is  \n",
    "- If too large ‚ûî may **overshoot** and miss the minimum  \n",
    "- If too small ‚ûî may result in **very slow** convergence\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Visual Intuition\n",
    "\n",
    "Imagine you are **walking downhill**:\n",
    "- The gradient tells you the **steepest direction** to walk\n",
    "- The learning rate controls **how big your steps are**\n",
    "\n",
    "‚úÖ Our goal is to **walk downhill** until we find the valley ‚Äî the **minimum loss**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2818fd",
   "metadata": {},
   "source": [
    "## üìö Section 8: Training Loop (Manual Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2d65cb",
   "metadata": {},
   "source": [
    "### üéØ Objective: Manual Training Loop\n",
    "\n",
    "Now it's time to put everything together and **manually implement a training loop**.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Your Task:\n",
    "\n",
    "- Use your **initialized parameters**: `W` and `b`  \n",
    "- Compute predictions using your `predict(X)` function  \n",
    "- Calculate the **loss** using Mean Squared Error (MSE)  \n",
    "- Perform **backpropagation** with `.backward()`  \n",
    "- Manually **update the parameters** using gradient descent:\n",
    "  \n",
    "  $$ \n",
    "  W \\leftarrow W - \\alpha \\cdot W.\\text{grad} \n",
    "  \\quad \\text{and} \\quad\n",
    "  b \\leftarrow b - \\alpha \\cdot b.\\text{grad}\n",
    "  $$\n",
    "\n",
    "- Don‚Äôt forget to **zero the gradients** after each update using:\n",
    "  ```python\n",
    "  W.grad.zero_()\n",
    "  b.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b9dcb",
   "metadata": {},
   "source": [
    "#### üß† Markdown Cell 8.1: The Structure of a Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c82cd96",
   "metadata": {},
   "source": [
    "# 8.1 Building the Training Loop\n",
    "\n",
    "‚úÖ In machine learning, a **training loop** is the repetitive process of:\n",
    "1. **Making predictions** using the current parameters.\n",
    "2. **Computing the loss** (difference between predictions and true targets).\n",
    "3. **Calculating gradients** (how loss changes w.r.t W and b).\n",
    "4. **Updating the parameters** using gradient descent.\n",
    "5. **Repeating** the process for many epochs (iterations).\n",
    "\n",
    "---\n",
    "\n",
    "üîµ **Typical Training Loop Steps:**\n",
    "\n",
    "For each epoch:\n",
    "- `y_pred = predict(X_train)`\n",
    "- `loss = mse_loss(y_pred, y_train)`\n",
    "- `loss.backward()`  ‚ûî compute gradients\n",
    "- Update `W` and `b`:\n",
    "  - `W = W - learning_rate * W.grad`\n",
    "  - `b = b - learning_rate * b.grad`\n",
    "- `zero out gradients` after update (important!)\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ In PyTorch, we update parameters inside a `torch.no_grad()` block to avoid tracking the updates in the computation graph.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0122f3a3",
   "metadata": {},
   "source": [
    "#### ‚úçÔ∏è Practice Task 8.1 (Training Loop Practice) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1073683a",
   "metadata": {},
   "source": [
    "#### üõ†Ô∏è Practice Exercise 8.1\n",
    "\n",
    "‚úÖ Your Task:\n",
    "\n",
    "Write a training loop to **train your linear regression model** on the Concrete dataset.\n",
    "\n",
    "Follow these steps:\n",
    "\n",
    "---\n",
    "\n",
    "1. Choose a **learning rate** (e.g., `0.01`).\n",
    "\n",
    "2. For a number of **epochs** (e.g., 100):\n",
    "\n",
    "    a. Make predictions using your `predict(X_train)` function.  \n",
    "    b. Compute the MSE loss using your `mse_loss()` function.  \n",
    "    c. Call `.backward()` on the loss to compute gradients.  \n",
    "    d. Update the weights `W` and bias `b`:\n",
    "       - Use `torch.no_grad()` block.\n",
    "       - Apply the gradient descent rule:\n",
    "         ```\n",
    "         W -= learning_rate * W.grad\n",
    "         b -= learning_rate * b.grad\n",
    "         ```\n",
    "    e. After updating, **zero out the gradients** using:\n",
    "       ```\n",
    "       W.grad.zero_()\n",
    "       b.grad.zero_()\n",
    "       ```\n",
    "\n",
    "---\n",
    "\n",
    "üîî *Hints:*\n",
    "- Use a `for epoch in range(epochs):` loop.\n",
    "- You can print the loss every 10 epochs to monitor progress.\n",
    "\n",
    "‚úÖ At the end, your W and b should have changed to better values!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8db5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "501567d4",
   "metadata": {},
   "source": [
    "#### üñ•Ô∏è Expected Solution (not shown to students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae259e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume W and b are already initialized, inputs split into X_train, y_train\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    y_pred = predict(X_train)\n",
    "    loss = mse_loss(y_pred, y_train)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    with torch.no_grad():\n",
    "        W -= learning_rate * W.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    # Zero gradients\n",
    "    W.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}: Loss = {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398c213d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e965b98f",
   "metadata": {},
   "source": [
    "## üìö Section 9: Plotting Loss over Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c45c22",
   "metadata": {},
   "source": [
    "### üéØ Objective: Visualize Loss Over Epochs\n",
    "\n",
    "- **Visualize** how the loss changes over training epochs  \n",
    "- Help students **see convergence** of the model (i.e., loss decreasing over time)  \n",
    "- Introduce basic **matplotlib plotting** to monitor training progress\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Your Task:\n",
    "\n",
    "1. Track the loss at each epoch in a list (e.g., `losses = []`)  \n",
    "2. After training, use `matplotlib.pyplot` to plot loss vs. epoch  \n",
    "3. Label your axes: `Epoch` (x-axis) and `Loss` (y-axis)  \n",
    "4. Add a title like `\"Training Loss Over Time\"`\n",
    "\n",
    "---\n",
    "\n",
    "üìå *This helps build intuition about when a model is learning and when it might be stuck!*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c085d",
   "metadata": {},
   "source": [
    "#### üß† Markdown Cell 9.1: Why Plot Training Loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839fcd2b",
   "metadata": {},
   "source": [
    "### 9.1 Why Plot Training Loss?\n",
    "\n",
    "‚úÖ During training, it‚Äôs important to **track the loss over time**:\n",
    "\n",
    "- If the loss **decreases smoothly**, your model is learning properly.\n",
    "- If the loss **bounces around wildly** or **increases**, there might be problems:\n",
    "  - Too large learning rate\n",
    "  - Bugs in your code\n",
    "  - Bad initialization\n",
    "\n",
    "---\n",
    "\n",
    "üîµ **Good training behavior:**\n",
    "- Loss starts high and **decreases steadily** toward a small value.\n",
    "\n",
    "üîµ **Bad training behavior:**\n",
    "- Loss increases, oscillates wildly, or gets stuck high.\n",
    "\n",
    "‚úÖ Visualizing the loss helps us **debug** and **understand training dynamics**!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4fbfa8",
   "metadata": {},
   "source": [
    "#### ‚úçÔ∏è Practice Task 9.1: Plot Loss (Students code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e642ca7",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Practice Exercise 9.1\n",
    "\n",
    "‚úÖ Your Task:\n",
    "\n",
    "- While training your model (Section 8), **store the loss value at every epoch**.\n",
    "- After training, **plot the loss curve** using matplotlib.\n",
    "\n",
    "---\n",
    "\n",
    "üîî *Hints:*\n",
    "- Create an empty list before training: `losses = []`\n",
    "- Inside the training loop, after computing loss, do: `losses.append(loss.item())`\n",
    "- After training:\n",
    "  ```python\n",
    "  import matplotlib.pyplot as plt\n",
    "  plt.plot(losses)\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.title('Training Loss over Epochs')\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa3831f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de6d58d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ‚úÖ Again, **Markdown gives enough instructions**, students must **write the code** properly.\n",
    "\n",
    "## üñ•Ô∏è Expected internal solution (not shown to students)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "losses = []\n",
    "\n",
    "# (inside training loop)\n",
    "losses.append(loss.item())\n",
    "\n",
    "# (after training)\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5896fe1a",
   "metadata": {},
   "source": [
    "## üìö Section 10: Model Evaluation (Train/Test MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435b0752",
   "metadata": {},
   "source": [
    "### üéØ Objective: Evaluate the Model After Training\n",
    "\n",
    "Now that training is complete, it's time to **evaluate the model's performance**.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Your Task:\n",
    "\n",
    "- Compute **Mean Squared Error (MSE)** on:\n",
    "  - ‚úÖ **Training set**\n",
    "  - ‚úÖ **Testing set**\n",
    "\n",
    "- Compare the two results:\n",
    "  - Is the model performing similarly on both?\n",
    "  - Or is it doing much better on training data?\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Key Concept: Overfitting vs. Underfitting\n",
    "\n",
    "- **Overfitting:** Model performs well on training data but poorly on unseen test data  \n",
    "- **Underfitting:** Model performs poorly on both training and test data (hasn't learned enough)\n",
    "\n",
    "üìå *Use this evaluation to reflect on your model's generalization ability.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335ba5ac",
   "metadata": {},
   "source": [
    "#### üß† Markdown Cell 10.1: Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7313c32f",
   "metadata": {},
   "source": [
    "### 10.1 Evaluating Model Performance\n",
    "\n",
    "‚úÖ After training a model, we need to **measure how well it performs**:\n",
    "\n",
    "- **Training MSE**: How well the model fits the data it was trained on.\n",
    "- **Testing MSE**: How well the model generalizes to unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "üîµ **Ideal Scenario:**\n",
    "- Low training MSE\n",
    "- Low testing MSE (slightly higher than training MSE)\n",
    "\n",
    "üîµ **Possible Problems:**\n",
    "- **Overfitting**: Very low training MSE but high testing MSE.\n",
    "- **Underfitting**: High training MSE and high testing MSE.\n",
    "\n",
    "‚úÖ Evaluating both gives us a **full picture** of model performance!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198ce49c",
   "metadata": {},
   "source": [
    "### ‚úçÔ∏è Practice Task 10.1: Compute Train and Test MSE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299dbc6d",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Practice Exercise 10.1\n",
    "\n",
    "‚úÖ Your Task:\n",
    "\n",
    "- After training your model:\n",
    "  1. Use your final trained `W` and `b`.\n",
    "  2. Predict on both:\n",
    "     - Training set (`X_train`)\n",
    "     - Testing set (`X_test`)\n",
    "  3. Compute the MSE loss for:\n",
    "     - Training predictions vs `y_train`\n",
    "     - Testing predictions vs `y_test`\n",
    "  4. Print both losses.\n",
    "\n",
    "---\n",
    "\n",
    "üîî *Hints:*\n",
    "- Reuse your `predict()` function and `mse_loss()` function.\n",
    "- You **don't need gradients** now (just predictions + loss).\n",
    "\n",
    "‚úÖ You will get two numbers:\n",
    "- Training Loss\n",
    "- Testing Loss\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388bf5b6",
   "metadata": {},
   "source": [
    "### üñ•Ô∏è Expected internal solution (not shown to students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a857190f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (824x8 and 1x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Predict on training set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_preds \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m mse_loss(train_preds, y_train)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Predict on testing set\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[33], line 13\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(X):\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (824x8 and 1x1)"
     ]
    }
   ],
   "source": [
    "# Predict on training set\n",
    "train_preds = predict(X_train)\n",
    "train_loss = mse_loss(train_preds, y_train)\n",
    "\n",
    "# Predict on testing set\n",
    "test_preds = predict(X_test)\n",
    "test_loss = mse_loss(test_preds, y_test)\n",
    "\n",
    "print(f'Training MSE: {train_loss.item():.4f}')\n",
    "print(f'Testing MSE: {test_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029047d6",
   "metadata": {},
   "source": [
    "## üìö Section 11: Reflection ‚Äî Limits of Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5bb79c",
   "metadata": {},
   "source": [
    "### üéØ Objective: Wrap-Up and Transition\n",
    "\n",
    "- Reflect on what we‚Äôve achieved using **linear regression**  \n",
    "- Realize the **limitations** of simple linear models  \n",
    "- Introduce the need for **activation functions** and **hidden layers** to capture complex patterns  \n",
    "- Prepare to **transition naturally** to upcoming notebooks on neural networks\n",
    "\n",
    "---\n",
    "\n",
    "### üîç What We've Achieved:\n",
    "\n",
    "- Built a working linear model from scratch\n",
    "- Understood gradient descent and manual training loops\n",
    "- Visualized training progress and evaluated model performance\n",
    "\n",
    "---\n",
    "\n",
    "### üöß Limitations of Linear Models:\n",
    "\n",
    "- Can only model **linear relationships**\n",
    "- Cannot capture **non-linear patterns** or **interactions**\n",
    "- Often **underfit** complex real-world datasets\n",
    "\n",
    "---\n",
    "\n",
    "### üîë Why Activation Functions and Hidden Layers?\n",
    "\n",
    "- **Activation functions** introduce non-linearity  \n",
    "- **Hidden layers** allow the model to learn more abstract representations  \n",
    "- Together, they form the foundation of **neural networks**\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Quick Check (Mini MCQs)\n",
    "\n",
    "**Q1.** Which of the following is a limitation of linear regression?  \n",
    "(A) It requires a loss function  \n",
    "(B) It only models linear relationships ‚úÖ  \n",
    "(C) It doesn't use any math  \n",
    "(D) It can't be implemented in PyTorch\n",
    "\n",
    "---\n",
    "\n",
    "**Q2.** Why do we need activation functions in a neural network?  \n",
    "(A) To reduce the training time  \n",
    "(B) To remove weights and biases  \n",
    "(C) To introduce non-linearity ‚úÖ  \n",
    "(D) To make the model linear\n",
    "\n",
    "---\n",
    "\n",
    "üéâ *Well done! You're now ready to explore neural networks and go beyond linear models.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfacf62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24b70080",
   "metadata": {},
   "source": [
    "### üß† Markdown Cell 11.1: How Well Did the Linear Model Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67fa188",
   "metadata": {},
   "source": [
    "## 11.1 Reflecting on Linear Models\n",
    "\n",
    "‚úÖ Congratulations! You have now:\n",
    "\n",
    "- Built a linear model from scratch.\n",
    "- Used gradient descent to minimize the MSE loss.\n",
    "- Evaluated model performance on training and testing data.\n",
    "\n",
    "---\n",
    "\n",
    "üîµ **Some Important Observations:**\n",
    "\n",
    "- A linear model can capture **linear relationships** between inputs and outputs.\n",
    "- But if the relationship is **nonlinear** (curved, complex patterns), a linear model **cannot fit well**.\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Key Limitations of Linear Models:\n",
    "\n",
    "| Limitation | Description |\n",
    "|:---|:---|\n",
    "| Cannot model nonlinear patterns | Only straight lines or flat planes |\n",
    "| Underfitting on complex data | Cannot capture intricate relationships |\n",
    "| No internal \"processing\" | Single-layer transformations only |\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **This is why** in Deep Learning, we often:\n",
    "- Add **hidden layers**.\n",
    "- Use **activation functions** (like ReLU, Sigmoid).\n",
    "- Stack **multiple layers** to build complex models.\n",
    "\n",
    "‚úÖ Hidden layers allow the network to **transform** and **learn nonlinear mappings** between input and output!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d1e584",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Practice Task 11.1: Quick MCQs (Knowledge Check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb715b0",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Practice Exercise 11.1 (MCQs)\n",
    "\n",
    "‚úÖ Choose the correct answers based on your learning so far:\n",
    "\n",
    "---\n",
    "\n",
    "**Q1. Why does a simple linear model fail on complex datasets?**\n",
    "\n",
    "- [ ] It memorizes the training data.\n",
    "- [x] It cannot capture nonlinear patterns.\n",
    "- [ ] It uses too many layers.\n",
    "- [ ] It overfits easily.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2. What is the main purpose of adding activation functions in a neural network?**\n",
    "\n",
    "- [x] To introduce nonlinearity\n",
    "- [ ] To make training faster\n",
    "- [ ] To remove bias\n",
    "- [ ] To reduce the number of parameters\n",
    "\n",
    "---\n",
    "\n",
    "**Q3. After training a model, a very low training loss but very high test loss suggests:**\n",
    "\n",
    "- [ ] Good generalization\n",
    "- [x] Overfitting\n",
    "- [ ] Underfitting\n",
    "- [ ] Correct model complexity\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ Write your selected answers clearly below each question.\n",
    "‚úÖ Reflect briefly (1‚Äì2 sentences) on **what you found easy or difficult** in building your first model!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8deacec",
   "metadata": {},
   "source": [
    "# üéØ Final Words for Notebook 2\n",
    "\n",
    "‚úÖ In this project, you have learned how to:\n",
    "\n",
    "- Build a basic linear model.\n",
    "- Train it using gradient descent.\n",
    "- Understand model evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "üöÄ Next, we will explore **Activation Functions** and **Hidden Layers** to move beyond simple linear models!\n",
    "\n",
    "Stay curious! üéØ‚ú®\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
